{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control random processes\n",
    "import torch\n",
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def transform(layout):\n",
    "    return np.log2(np.where(layout==0, 1, layout))/11-.25\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, inputSize=16, outputSize=4, neuronCountJ=200, neuronCountK=100):\n",
    "        # initialize network\n",
    "        self.model = nn.Sequential(nn.Linear(inputSize, neuronCountJ),\n",
    "                       nn.ReLU(), \n",
    "                       nn.Linear(neuronCountJ, neuronCountK),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(neuronCountK, outputSize),\n",
    "                       nn.Softmax(dim=1),\n",
    "                     )\n",
    "        self.model.double()\n",
    "        \n",
    "    def random_init(self, max_tile=None):\n",
    "        # generates a random initial layout, possibly given a max tile for the layout to build off of\n",
    "        ## ex: max tile=64 will generate a 4x4 layout where the largest tile is a 64, all other tile values are smaller\n",
    "        if max_tile:\n",
    "            max_tile = int(np.log2(max_tile))\n",
    "            assert max_tile in range(0,10) # maximum tile cannot be larger than 512\n",
    "        else:\n",
    "            max_tile = np.random.choice(range(3,10))\n",
    "        goal_tile = 2**(max_tile+1)\n",
    "\n",
    "        # insert the max tile into the matrix\n",
    "        init_layout = np.zeros((4,4), dtype=np.int)\n",
    "        init_layout[np.random.choice(range(4)), np.random.choice(range(4))] = 2**max_tile\n",
    "\n",
    "        for move in range(np.random.choice(range(4,12))):\n",
    "            row_idx = np.random.choice(np.where((init_layout==0).sum(axis=1)>0)[0])\n",
    "            init_layout[row_idx, np.random.choice(np.where(init_layout[row_idx]==0)[0])] = 2**np.random.choice(range(1,max_tile))\n",
    "\n",
    "        return init_layout, goal_tile\n",
    "\n",
    "    def get_game_idx(self, data, game_metric=None):\n",
    "        if game_metric=='tile_sum': # micro-game with largest ending tile sum\n",
    "            metric = data.tile_sums\n",
    "            metric[np.where(~data.wins)] = 0\n",
    "            return np.argmax(metric)\n",
    "        elif game_metric=='moves': # micro-game with lowest # of moves\n",
    "            metric = data.num_moves\n",
    "            metric[np.where(~data.wins)] = metric.max()+1\n",
    "            return np.argmin(metric)\n",
    "        else: # default to the micro-game with highest score\n",
    "            metric = data.final_scores\n",
    "            metric[np.where(~data.wins)] = 0\n",
    "            return np.argmax(metric)\n",
    "\n",
    "    def compute_game_penalties(self, data, penalty_type):\n",
    "        # all weights range [-1,1], where weight<0 indicates a \"bad\" game and weight>=0 indicates a \"good\" game\n",
    "        if penalty_type=='scores':\n",
    "            # computed by overall game score\n",
    "            rank_values = data.final_scores\n",
    "        elif penalty_type=='max':\n",
    "            # computed by max tile on board at the end of game\n",
    "            rank_values = data.max_tile\n",
    "        elif penalty_type=='log2_max':\n",
    "            # computed by the base 2 log of max tile on board at the end of game        \n",
    "            rank_values = np.log2(data.max_tile)\n",
    "        elif penalty_type=='tile_sums':\n",
    "            # computed by sum of tile values at the end of game\n",
    "            rank_values = data.tile_sums\n",
    "        else: # use binary as default (-1:final score was below median, 1: final score was above median)\n",
    "            penalties = np.ones(data.final_scores.shape)\n",
    "            penalties[data.final_scores<=np.median(data.final_scores)] = -1  \n",
    "        \n",
    "        if penalty_type is not None: # runs for all except default (binary)\n",
    "            # using distance to median by whatever metric was chosen using penalty_type\n",
    "            maxes = np.repeat(rank_values.max(), rank_values.shape)\n",
    "            maxes[rank_values<=np.median(rank_values)] = rank_values.min()\n",
    "            maxes = np.absolute(maxes-np.median(rank_values))\n",
    "            penalties = (rank_values-np.median(rank_values))/maxes\n",
    "    \n",
    "        return penalties\n",
    "\n",
    "    def compute_move_penalties(self, data, penalty_type):\n",
    "        # all weights range [0,1]\n",
    "        if penalty_type=='nonzero':\n",
    "            # weights by fraction of tiles that are nonzero\n",
    "            weights = np.count_nonzero(np.concatenate(data.layouts), axis=1)/16\n",
    "        elif penalty_type=='linear_move_num':\n",
    "            # weights linearly by move number (move #/total # of moves)\n",
    "            weights = np.concatenate([np.linspace(0,1,num_moves) for num_moves in data.num_moves])\n",
    "        elif penalty_type=='exponential_move_num':\n",
    "            # weights exponentially (1-e^(-3x)) by move number where x=(move #/total # of moves)\n",
    "            weights = np.concatenate([1-np.exp(-3*np.linspace(0, 1, num=num_moves)) for num_moves in data.num_moves])\n",
    "        else:\n",
    "            # weight all moves equally\n",
    "            weights = np.ones(data.num_moves.sum())\n",
    "        return weights\n",
    "\n",
    "    def get_data(self, init_layout=None, goal_tile=2048, batch_size=10, random_frac=None, random_games=0):\n",
    "        \n",
    "        # define method for training (possibly including random moves with neural network-selected moves)\n",
    "        if random_frac is not None:\n",
    "            method = lambda layout: self.model(torch.from_numpy(transform(layout)).double().reshape(1,-1)).detach().numpy().flatten() if np.random.random()>random_frac else np.repeat(.25, 4)\n",
    "        else:\n",
    "            method = lambda layout: self.model(torch.from_numpy(transform(layout)).double().reshape(1,-1)).detach().numpy().flatten()\n",
    "        \n",
    "        # initialize games class\n",
    "        from helper import GameDriver\n",
    "        data = GameDriver()\n",
    "        \n",
    "        # default option is to run entire games\n",
    "        if init_layout is not None:\n",
    "            # run neural-network-run games with initialization\n",
    "            data.run_games(batch_size, method=method, init_layout=init_layout,\n",
    "                          early_stop=goal_tile)\n",
    "            if random_games>0: # run same number of completely random games, if applicable\n",
    "                data.run_games(random_games, init_layout=init_layout,\n",
    "                              early_stop=goal_tile) \n",
    "        else:\n",
    "            data.run_games(batch_size, method=method,\n",
    "                          early_stop=goal_tile) # run neural-network-run games\n",
    "            if random_games>0:\n",
    "                data.run_games(random_games,\n",
    "                          early_stop=goal_tile) # run some number of completely random games, if applicable\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def train(self, lr=0.001, duration=1/600, random_games=False, random_frac=None, batch_size=10,\n",
    "             move_penalty_type=None, game_penalty_type=None, game_type='full',\n",
    "             test=False):\n",
    "        \n",
    "        # save all parameters passed to train() in the model object for later serialization\n",
    "        self.model.user_parameters = locals().copy()\n",
    "        self.model.user_parameters.pop('self')\n",
    "        \n",
    "        # initialize optimizer and loss function\n",
    "        opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        loss = nn.L1Loss()\n",
    "        \n",
    "        # initialize variables to hold data during training\n",
    "        end_time = time.time()+60*60*duration\n",
    "        self.model.scores = defaultdict(list)\n",
    "        \n",
    "        if game_type=='mini_iterative' or game_type=='mini_random':\n",
    "            max_idx_start=3\n",
    "        \n",
    "        while time.time()<end_time: # run loop for a certain duration (in hours)\n",
    "            \n",
    "            # ------------------ get appropriate game data ------------------------\n",
    "            if game_type=='full':\n",
    "                data = self.get_data(batch_size=batch_size, random_frac=random_frac, random_games=random_games)\n",
    "                \n",
    "            else:\n",
    "                if game_type=='mini_random':\n",
    "                    \n",
    "                    # choose a max tile for random initial layout (draw from uniform distribution)\n",
    "                    random_idx_start = np.random.choice(range(3,max_idx_start+1))\n",
    "                    init_layout, goal_tile = self.random_init(max_tile=2**random_idx_start)\n",
    "                    \n",
    "                    # run mini games with random initial layout\n",
    "                    data = self.get_data(init_layout, goal_tile, batch_size, random_frac, random_games)\n",
    "                    \n",
    "                    # if at least 80% of games were successfully played\n",
    "                    ## AND we randomly drew the largest possible index, extend range of indexes available\n",
    "                    ## (limited to 10, meaning we can never initialize with a tile greater than 1048)\n",
    "                    if data.wins.mean()>.8 and max_idx_start==random_idx_start:\n",
    "                        max_idx_start = min(10, max_idx_start+1)\n",
    "                    return data\n",
    "                \n",
    "                elif game_type=='mini_iterative':\n",
    "                    \n",
    "                    if max_idx_start==3: # need to start with a random layout (new round)\n",
    "                        init_layout, goal_tile = self.random_init(max_tile=2**max_idx_start)\n",
    "                    \n",
    "                    for attempt in range(10):\n",
    "                        # run mini games with random layout OR layout left over from last pass\n",
    "                        #return init_layout, goal_tile, batch_size, random_frac, random_games\n",
    "                        data = self.get_data(init_layout=init_layout, goal_tile=goal_tile, batch_size=batch_size, \n",
    "                                             random_frac=random_frac, random_games=random_games)\n",
    "                        \n",
    "                        if data.wins.max(): # if at least one mini game was successful\n",
    "                            # set up initial layout for next pass \n",
    "                            ## (increment maximum tile, choose appropriate layout for initialization)\n",
    "                            max_idx_start = max(3, (max_idx_start+1)%11)\n",
    "                            if max_idx_start>3:\n",
    "                                init_layout = data.final_layouts[self.get_game_idx(data)]\n",
    "                                goal_tile = 2**(max_idx_start+1)\n",
    "                            break\n",
    "                            \n",
    "                        if attempt==9: # couldn't make it to next tile from this layout after 10 attempts\n",
    "                            max_idx_start = 3 # start a new round\n",
    "                            \n",
    "            \n",
    "            # log performance of training epoch\n",
    "            for i in range(3,12):\n",
    "                self.model.scores[2**i].append(np.count_nonzero(data.max_tile==2**i))            \n",
    "                        \n",
    "            if test:\n",
    "                continue\n",
    "                \n",
    "            # ------------------ use data to update model weights ------------------------                            \n",
    "            # find weights for good/bad game performance and weights for move importance\n",
    "            game_penalties = self.compute_game_penalties(data, game_penalty_type)\n",
    "            move_penalties = self.compute_move_penalties(data, move_penalty_type)\n",
    "            expanded_game_penalties = np.repeat(game_penalties, data.num_moves)\n",
    "            \n",
    "            # set up data to train\n",
    "            L = torch.from_numpy(transform(np.concatenate(data.layouts))).double()\n",
    "            M = torch.from_numpy(np.concatenate(data.moves)).double()\n",
    "            y_hat = self.model(L)\n",
    "            \n",
    "            # make \"true\" labels - M where the game was \"good\", (1-M)/3 where the game was \"bad\"\n",
    "            y = M.clone()\n",
    "            mask = expanded_game_penalties<0\n",
    "            mask = torch.from_numpy(mask).nonzero().flatten()\n",
    "            y[mask,:] = (1-y[mask,:])/3        \n",
    "        \n",
    "            # align penalties for torch compatibility\n",
    "            ## note absolute value for game penalties, since directionality was taken care of when generating true labels\n",
    "            expanded_game_penalties = torch.abs(torch.from_numpy(expanded_game_penalties)[:,None].double())\n",
    "            move_penalties = torch.from_numpy(move_penalties)[:,None].double()\n",
    "                  \n",
    "            # update model weights\n",
    "            output = loss(expanded_game_penalties*move_penalties*y, expanded_game_penalties*move_penalties*y_hat)\n",
    "            output.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        if test:\n",
    "            return\n",
    "                \n",
    "        # call log_results to save the trained model\n",
    "        self.log_results()\n",
    "            \n",
    "    def log_results(self):\n",
    "        import os\n",
    "        directory='.\\\\model_results'\n",
    "        \n",
    "        # generate folder for models, if necessary\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        # save model\n",
    "        model_int = 0\n",
    "        filename = os.path.join(directory, 'model_{}.pickle'.format(model_int))\n",
    "        \n",
    "        while os.path.isfile(filename):\n",
    "            model_int += 1\n",
    "            filename = os.path.join(directory, 'model_{}.pickle'.format(model_int))\n",
    "        \n",
    "        import pickle\n",
    "        pickle.dump(self.model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'game_type':['full', 'mini_iterative', 'mini_random'],\n",
    "         'random_games':[False, True],\n",
    "         'random_frac':[None, np.random.uniform(0, 1)],\n",
    "         'batch_size':10,\n",
    "          'lr':0.001,\n",
    "         'move_penalty_type':[None,'nonzero','linear_move_num','exponential_move_num'],\n",
    "         'game_penalty_type':[None,'scores','max','log2_max','tile_sums']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test parameter combinations\n",
    "\n",
    "for game_type in params['game_type']:\n",
    "    for random_games in params['random_games']:\n",
    "        for random_frac in params['random_frac']:\n",
    "            for move_penalty_type in params['move_penalty_type']:\n",
    "                for game_penalty_type in params['game_penalty_type']:\n",
    "                    network.train(test=True, duration=1/60000,\n",
    "                                 game_type=game_type, random_games=random_games, random_frac=random_frac,\n",
    "                                 move_penalty_type=move_penalty_type, game_penalty_type=game_penalty_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
