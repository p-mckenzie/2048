{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control random processes\n",
    "import torch\n",
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import time\n",
    "\n",
    "def transform(layout):\n",
    "    return np.log2(np.where(layout==0, 1, layout))/11-.25\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, inputSize=16, outputSize=4, neuronCountJ=200, neuronCountK=100):\n",
    "        # initialize network\n",
    "        self.model = nn.Sequential(nn.Linear(inputSize, neuronCountJ),\n",
    "                       nn.ReLU(), \n",
    "                       nn.Linear(neuronCountJ, neuronCountK),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(neuronCountK, outputSize),\n",
    "                       nn.Softmax(dim=1),\n",
    "                     )\n",
    "        self.model.double()\n",
    "        \n",
    "    def random_init(self, max_tile=None):\n",
    "        # generates a random initial layout, possibly given a max tile for the layout to build off of\n",
    "        ## ex: max tile=64 will generate a 4x4 layout where the largest tile is a 64, all other tile values are smaller\n",
    "        if max_tile:\n",
    "            max_tile = int(np.log2(max_tile))\n",
    "            assert max_tile in range(0,10) # maximum tile cannot be larger than 512\n",
    "        else:\n",
    "            max_tile = np.random.choice(range(3,10))\n",
    "        goal_tile = 2**(max_tile+1)\n",
    "\n",
    "        # insert the max tile into the matrix\n",
    "        init_layout = np.zeros((4,4), dtype=np.int)\n",
    "        init_layout[np.random.choice(range(4)), np.random.choice(range(4))] = 2**max_tile\n",
    "\n",
    "        for move in range(np.random.choice(range(4,12))):\n",
    "            row_idx = np.random.choice(np.where((init_layout==0).sum(axis=1)>0)[0])\n",
    "            init_layout[row_idx, np.random.choice(np.where(init_layout[row_idx]==0)[0])] = 2**np.random.choice(range(1,max_tile))\n",
    "\n",
    "        return init_layout, goal_tile\n",
    "\n",
    "    def get_game_idx(self, data, game_metric=None):\n",
    "        if game_metric=='tile_sum': # micro-game with largest ending tile sum\n",
    "            return np.argmax(data.tile_sums)\n",
    "        elif game_metric=='moves': # micro-game with lowest # of moves\n",
    "            return np.argmin(data.num_moves)\n",
    "        else: # default to the micro-game with highest score\n",
    "            return np.argmax(data.final_scores)\n",
    "\n",
    "    def best_init(self, data, game_idx, max_tile=None):\n",
    "        possible_max = int(np.log2(data.layouts[game_idx].max()))\n",
    "        if max_tile:\n",
    "            max_tile = int(np.log2(max_tile))\n",
    "            assert max_tile<possible_max # maximum tile must be feasible given the micro-game we've chosen\n",
    "        else: # default to the second largest tile the micro-game ever reaches\n",
    "            max_tile = possible_max-1\n",
    "\n",
    "        goal_tile = 2**max_tile\n",
    "        print(max_tile)\n",
    "        # select the first time the max_tile appears in the game corresponding to game_idx\n",
    "        init_layout = data.layouts[game_idx][np.where(data.layouts[game_idx].max(axis=1)==int(2**max_tile))[0][0]].reshape((4,4))\n",
    "        goal_tile = 2**int((np.log2(init_layout.max())+1))\n",
    "\n",
    "        return init_layout, goal_tile\n",
    "\n",
    "    def compute_game_penalties(self, data, penalty_type):\n",
    "        # all weights range [-1,1], where weight<0 indicates a \"bad\" game and weight>=0 indicates a \"good\" game\n",
    "        if penalty_type=='scores':\n",
    "            # computed by overall game score\n",
    "            rank_values = data.final_scores\n",
    "        elif penalty_type=='max':\n",
    "            # computed by max tile on board at the end of game\n",
    "            rank_values = data.max_tile\n",
    "        elif penalty_type=='log2_max':\n",
    "            # computed by the base 2 log of max tile on board at the end of game        \n",
    "            rank_values = np.log2(data.max_tile)\n",
    "        elif penalty_type=='tile_sums':\n",
    "            # computed by sum of tile values at the end of game\n",
    "            rank_values = data.tile_sums\n",
    "        else: # use binary as default (-1:final score was below median, 1: final score was above median)\n",
    "            penalties = np.ones(data.final_scores.shape)\n",
    "            penalties[data.final_scores<=np.median(data.final_scores)] = -1  \n",
    "        \n",
    "        if penalty_type is not None: # runs for all except default (binary)\n",
    "            # using distance to median by whatever metric was chosen using penalty_type\n",
    "            maxes = np.repeat(rank_values.max(), rank_values.shape)\n",
    "            maxes[rank_values<=np.median(rank_values)] = rank_values.min()\n",
    "            maxes = np.absolute(maxes-np.median(rank_values))\n",
    "            penalties = (rank_values-np.median(rank_values))/maxes\n",
    "    \n",
    "        return penalties\n",
    "\n",
    "    def compute_move_penalties(self, data, penalty_type):\n",
    "        # all weights range [0,1]\n",
    "        if penalty_type=='nonzero':\n",
    "            # weights by fraction of tiles that are nonzero\n",
    "            weights = np.count_nonzero(np.concatenate(data.layouts), axis=1)/16\n",
    "        elif penalty_type=='linear_move_num':\n",
    "            # weights linearly by move number (move #/total # of moves)\n",
    "            weights = np.concatenate([np.linspace(0,1,num_moves) for num_moves in data.num_moves])\n",
    "        elif penalty_type=='exponential_move_num':\n",
    "            # weights exponentially (1-e^(-3x)) by move number where x=(move #/total # of moves)\n",
    "            weights = np.concatenate([1-np.exp(-3*np.linspace(0, 1, num=num_moves)) for num_moves in data.num_moves])\n",
    "        else:\n",
    "            # weight all moves equally\n",
    "            weights = np.ones(data.num_moves.sum())\n",
    "        return weights\n",
    "\n",
    "    def train(self, lr=0.001, duration=1/600, random_games=None, random_frac=None, batch_size=10,\n",
    "             move_penalty_type=None, game_penalty_type=None, game_type='full',\n",
    "             test=False):\n",
    "        # save parameters in the model object for later serialization\n",
    "        self.model.lr = lr\n",
    "        self.model.duration = duration\n",
    "        self.model.random_games = random_games\n",
    "        self.model.random_frac = random_frac\n",
    "        self.model.batch_size = batch_size\n",
    "        self.model.move_penalty_type = move_penalty_type\n",
    "        self.model.game_penalty_type = game_penalty_type\n",
    "        self.model.game_type = game_type\n",
    "        \n",
    "        # initialize optimizer and loss function\n",
    "        opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        loss = nn.L1Loss()\n",
    "        \n",
    "        # actual game class (to run games and get data)\n",
    "        from helper import GameDriver\n",
    "\n",
    "        # define method for training (possibly including random moves with neural network-selected moves)\n",
    "        if random_frac is not None:\n",
    "            method = lambda layout: self.model(torch.from_numpy(transform(layout)).double().reshape(1,-1)).detach().numpy().flatten() if np.random.random()>random_frac else np.repeat(.25, 4)\n",
    "        else:\n",
    "            method = lambda layout: self.model(torch.from_numpy(transform(layout)).double().reshape(1,-1)).detach().numpy().flatten()\n",
    "        \n",
    "        #\n",
    "        if game_type=='mini_iterative':\n",
    "            max_idx = 3\n",
    "        \n",
    "        # initialize variables to hold data during training\n",
    "        end_time = time.time()+60*60*duration\n",
    "        scores = []\n",
    "        while time.time()<end_time: # run loop for a certain duration (in hours)\n",
    "            # initialize games class\n",
    "            data = GameDriver()\n",
    "            \n",
    "            # run single-goal micro-games, with random initialization\n",
    "            if game_type=='mini_random':\n",
    "                init_layout, goal_tile = self.random_init()\n",
    "                # run neural-network-run games\n",
    "                data.run_games(batch_size, method=method, init_layout=init_layout,\n",
    "                              early_stop=goal_tile) \n",
    "                if random_games: # run some number of completely random games, if applicable\n",
    "                    data.run_games(int(batch_size*random_frac), init_layout=init_layout,\n",
    "                                  early_stop=goal_tile) \n",
    "                    \n",
    "            # run single-goal micro-games, with initialization based on successful games\n",
    "            elif game_type=='mini_iterative':\n",
    "                # make sure the maximum tile is always between 8 (2**3) and 1024 (2**10)\n",
    "                max_idx = max(3,max_idx%11)\n",
    "                print(max_idx)\n",
    "                \n",
    "                if max_idx==3: # need random start\n",
    "                    init_layout, goal_tile = self.random_init()\n",
    "                else: # use \"best\" end point from last run as initialization\n",
    "                    init_layout, goal_tile = self.best_init(data, self.get_game_idx(data, game_metric=None), max_tile=2**max_idx)\n",
    "                \n",
    "                # run neural-network-run games\n",
    "                data.run_games(batch_size, method=method, init_layout=init_layout,\n",
    "                              early_stop=goal_tile)\n",
    "                if random_games:\n",
    "                    data.run_games(int(batch_size*random_frac), init_layout=init_layout,\n",
    "                              early_stop=goal_tile) # run some number of completely random games, if applicable\n",
    "\n",
    "\n",
    "                max_idx += 1\n",
    "            \n",
    "            # default option is to run entire games\n",
    "            else:\n",
    "                data.run_games(batch_size, method=method) # run neural-network-run games\n",
    "                if random_games:\n",
    "                    data.run_games(int(batch_size*random_frac)) # run some number of completely random games, if applicable\n",
    "            \n",
    "                \n",
    "            # find weights for good/bad game performance and weights for move importance\n",
    "            game_penalties = self.compute_game_penalties(data, game_penalty_type)\n",
    "            move_penalties = self.compute_move_penalties(data, move_penalty_type)\n",
    "            expanded_game_penalties = np.repeat(game_penalties, data.num_moves)\n",
    "            \n",
    "            # set up data to train\n",
    "            L = torch.from_numpy(transform(np.concatenate(data.layouts))).double()\n",
    "            M = torch.from_numpy(np.concatenate(data.moves)).double()\n",
    "            y_hat = self.model(L)\n",
    "            \n",
    "            # make \"true\" labels - M where the game was \"good\", (1-M)/3 where the game was \"bad\"\n",
    "            y = M.clone()\n",
    "            mask = expanded_game_penalties<0\n",
    "            mask = torch.from_numpy(mask).nonzero().flatten()\n",
    "            y[mask,:] = (1-y[mask,:])/3        \n",
    "        \n",
    "            # align penalties for torch compatibility\n",
    "            ## note absolute value for game penalties, since directionality was taken care of when generating true labels\n",
    "            expanded_game_penalties = torch.abs(torch.from_numpy(expanded_game_penalties)[:,None].double())\n",
    "            move_penalties = torch.from_numpy(move_penalties)[:,None].double()\n",
    "                  \n",
    "            if test:\n",
    "                return\n",
    "        \n",
    "            # update model weights\n",
    "            output = loss(expanded_game_penalties*move_penalties*y, expanded_game_penalties*move_penalties*y_hat)\n",
    "            output.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        # save scores in the model object for later serialization\n",
    "        self.model.scores = scores\n",
    "        \n",
    "        # call log_results to save the trained model\n",
    "        self.log_results()\n",
    "            \n",
    "    def log_results(self):\n",
    "        import os\n",
    "        directory='.\\\\model_results'\n",
    "        \n",
    "        # generate nested folder structure: folder for all models, then subfolder for this model\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            model_num = 0\n",
    "        else:\n",
    "            model_num = int(max(os.listdir(directory)))+1\n",
    "        subdirectory = os.path.join(directory, str(model_num))\n",
    "        os.makedirs(subdirectory)\n",
    "        \n",
    "        # save model, scores (including a graph), parameters\n",
    "        import pickle\n",
    "        pickle.dump(self.model, open(os.path.join(subdirectory, 'model.pickle'), 'wb'))\n",
    "        \n",
    "        # save graph of scores\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        def moving_average(a, n=3) :\n",
    "            ret = np.cumsum(a, dtype=float)\n",
    "            ret[n:] = ret[n:] - ret[:-n]\n",
    "            return ret[n - 1:] / n\n",
    "\n",
    "        a = moving_average(np.array(self.model.scores), 30)\n",
    "        plt.plot(a)\n",
    "        plt.title(\"Average game score during training\")\n",
    "        plt.ylabel('Game score')\n",
    "        plt.xlabel('Epoch')\n",
    "\n",
    "        plt.savefig(open(os.path.join(subdirectory, 'results.png'), 'wb'), dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GameDriver' object has no attribute 'final_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-08d7b929d2c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mini_iterative'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-3862dbd3a1e2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, lr, duration, random_games, random_frac, batch_size, move_penalty_type, game_penalty_type, game_type, test)\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0minit_layout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal_tile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# use \"best\" end point from last run as initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                     \u001b[0minit_layout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal_tile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_game_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmax_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;31m# run neural-network-run games\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-3862dbd3a1e2>\u001b[0m in \u001b[0;36mget_game_idx\u001b[1;34m(self, data, game_metric)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_moves\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# default to the micro-game with highest score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbest_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GameDriver' object has no attribute 'final_scores'"
     ]
    }
   ],
   "source": [
    "network.train(game_type='mini_iterative')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
